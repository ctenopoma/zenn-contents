---
title: "ヒト介入型の強化学習:HACO Vol.1"
emoji: "🎁"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["強化学習", "HumanInTheLoop", "Python"]
published: false
---
## 1. はじめに

 人が介入することで、効率的に強化学習を行う手法である、HACOについて調べてみた。
 @[card](https://decisionforce.github.io/HACO/)

### 1.1 Human-in-the-loopとは

 Human-in-the-loopとは、～

:::details 記事一覧

#### Human-in-the-loop について

@[card](http://localhost:8000/articles/human-in-the-loop_survey_no1)
:::

#### 1.2 目次

- [1. はじめに](#1-はじめに)
  - [1.1 Human-in-the-loopとは](#11-human-in-the-loopとは)
    - [Human-in-the-loop について](#human-in-the-loop-について)
    - [1.2 目次](#12-目次)
  - [1.3 Abstract](#13-abstract)
- [2. Human-AI](#2-human-ai)
  - [2.1 安全性が求められる環境下での学習](#21-安全性が求められる環境下での学習)
    - [2.1.1 模倣学習（IL）](#211-模倣学習il)
    - [2.1.2 先行研究におけるhuman-in-the-loop](#212-先行研究におけるhuman-in-the-loop)
    - [2.1.3 提案手法](#213-提案手法)
  - [2.2 関連手法](#22-関連手法)
    - [2.2.1 試行から学習する手法](#221-試行から学習する手法)
    - [Vol.2はこちら](#vol2はこちら)

### 1.3 Abstract

 人が持つ知識を強化学習の学習過程に組み込むことで「素早い学習」と「安全な試行」を実現することができるが、agentの学習過程において人が介入する機会は限られており、学習にうまく反映することは困難である。

 HACOは、agentが危険な環境において学習の安全性を確保しながら十分な探索を行えるようにするために、危険な状況や些細な行動を回避する方法をagentに示すために、人間の専門家が制御を代行することができる。
 HACOの特徴を以下に示す。

- 試行錯誤により得たデータと、人の介入によって得たデータを効率的に利用して学習を行う
  →  章
- 代理的行動価値(proxy state-action values)を人の操作から抽出し、人の介入が減るように学習を行う
  →  章
- 環境報酬が必要ない
  →  章

 結果として、安全な試行を維持しながら高いサンプル効率を有することが示されており、未知の環境における運転タスクにて「強化学習」と「模倣学習」の両方の面において、良い成績に達することができた。
  →  章

## 2. Human-AI

### 2.1 安全性が求められる環境下での学習

高い安全性が求められる環境下でのagentの学習に人の知識を介入させること
→強化学習では考え練られた報酬関数に、人の意図をどうやって反映するかが重要となるが、2つの問題点をはらんでいる。

:::message alert

強化学習の持つ問題点

1. 試行錯誤的な探索では、agentは不安全な行動をとること
1. 複数の意図的な行動（大回り、緊急ブレーキ、etc）を１つの報酬関数にまとめて学習することは難しい

:::

#### 2.1.1 模倣学習（IL）

これらの2つの課題に対して、人（エキスパート）の生成した状態とアクションシーケンスを模倣するように訓練される模倣学習（IL）では、agentの取る行動に対してエキスパートの意図を強制的に介入させている。
学習中の未熟なagentは、安全性が求められる環境においてエキスパートの介入より危険な行動を取らなくなるため、試行の安全性が確保される。

しかし、ILパラダイムは分布シフトの問題と、誘導されたスキルは制御タスの変化に対して十分なロバスト性を有していないとの問題がある。

#### 2.1.2 先行研究におけるhuman-in-the-loop

Human-in-the-loopは純粋なRLやILとは異なり、人がagentの学習過程を監視しながら介入する手法である。
先行研究でのHuman-in-the-loopは、agentの取った行動の評価や、軌道の選択に関して人の介入を行っていたが、それでは学習過程において不安全な試行が行われる可能性がある。
他の研究では、探索を中止させることで人の介入を行う手法があるが、実際には途中で探索を終了させて環境を元に戻すことは現実的ではない。

人がagentに介入して制御を引き継ぐことは、Human－AIシステムの安全性を守るための自然なアプローチであるが、先行研究では人が介入する機会に問題があると示唆されている。
人が介入する機会は限られているため、いつ、どうやって介入し、効率的にagentを学習させるかは重要な課題である。

#### 2.1.3 提案手法

HACOの重要な性質は、人の介入を最小限に、かつ試行中における学習中agentの自動化のレベルを調整できる点である。
学習に用いるのは2つのデータ
1. 人の介入により得られたデータ
1. 部分的な試行データ

？学習中にagentは環境報酬にアクセスすることができないにも関わらず、オフライン強化学習を用いて人とAIの混合した代理価値関数を維持する。

### 2.2 関連手法

#### 2.2.1 試行から学習する手法

- Passive imitation learning、オフライン強化学習
 ⇒ 環境との相互作用が不要なため、既製のデータセットからagentを学習し、学習中の試行の安全性を保証する手法。

- 逆強化学習
 ⇒ 人の試行から報酬関数を学習し、agentに模倣させるために使用する。

- GAIL、SQIL
 ⇒ 人とAIが出力する軌跡の類似度を比較し、agentに環境と作用するように要求する。

(Vol.1　完)

 #### Vol.2はこちら
 @[card](http://localhost:8000/articles/human_in_the_loop-haco_no2)
